{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "nlp_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcFR8EacppUp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR5mEik19lj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Supress unnecessary warnings so that presentation looks clean\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVLF3hXNe4b9",
        "colab_type": "code",
        "outputId": "52f97282-37c2-4c82-8c7e-b4e4f426f2c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsyHONuffyvp",
        "colab_type": "code",
        "outputId": "d1ad86e0-1667-4106-d2f8-ef8ece4b76c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Google Colab stuff\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_SwTXvse4cK",
        "colab_type": "code",
        "outputId": "442b9ca7-76de-490d-e579-10bc2cf3fb30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# create a dataframe using texts and lables\n",
        "trainDF = train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/NLP/data/training_data.csv')\n",
        "trainDF.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>text</th>\n",
              "      <th>ideas_content</th>\n",
              "      <th>organization</th>\n",
              "      <th>voice</th>\n",
              "      <th>word_choice</th>\n",
              "      <th>sentence_fluency</th>\n",
              "      <th>conventions</th>\n",
              "      <th>Unnamed: 8</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>Unnamed: 10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A long time ago when I was in third grade I h...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Softball has to be one of the single most gre...</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Some people like making people laugh, I love ...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>\"LAUGHTER\"  @CAPS1 I hang out with my friends...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Well ima tell a story about the time i got @CA...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   essay_id  ... Unnamed: 10\n",
              "0         1  ...         NaN\n",
              "1         2  ...         NaN\n",
              "2         3  ...         NaN\n",
              "3         4  ...         NaN\n",
              "4         5  ...         NaN\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4s0xoAag399",
        "colab_type": "code",
        "outputId": "dafb66e4-c2aa-46e2-977d-af2f8717cfa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQDlhxcSgud1",
        "colab_type": "code",
        "outputId": "f7d877d9-93e1-44a6-f110-c93294fbcd15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(self, path):\n",
        "\n",
        "        self.df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/NLP/data/training_data.csv\")\n",
        "        self.data = self.df.to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "        self.stopwords = set(stopwords.words('english'))\n",
        "        self.essay_id = self.data[:,0]\n",
        "        self.text = self.data[:,1]\n",
        "        self.scores = self.data[:,2:8]\n",
        "        self.new_data = []\n",
        "        self.new_scores = []\n",
        "        self.vocab = set()\n",
        "        self.word_to_id = None\n",
        "\n",
        "    def preprocess(self):\n",
        "        for i in range(len(self.essay_id)):\n",
        "            text = self.text[i].lower()\n",
        "            text = \" \".join([word for word in text.split() if '@' not in word])\n",
        "            text = word_tokenize(text)\n",
        "            text = [word for word in text if word not in self.stopwords]\n",
        "            self.text[i] = text \n",
        "\n",
        "    def create_vocab(self):\n",
        "        for line in self.text:\n",
        "            for word in line:\n",
        "                self.vocab.add(word)\n",
        "\n",
        "        self.vocab = sorted(list(self.vocab))\n",
        "        self.word_to_id = {word:i for i, word in enumerate(self.vocab)}\n",
        "\n",
        "    def text_num(self):\n",
        "        for i, line in enumerate(self.text):\n",
        "            x = []\n",
        "            for word in line:\n",
        "                x.append(self.word_to_id[word])\n",
        "            self.text[i] = x\n",
        "\n",
        "    def create_chunks(self):\n",
        "        for idx in range(len(self.essay_id)):\n",
        "            ess = self.text[idx]\n",
        "            n = len(ess)\n",
        "            self.new_data.append([ess[:n//3]])\n",
        "            self.new_data.append([ess[n//3:2*n//3]])\n",
        "            self.new_data.append([ess[2*n//3:]])\n",
        "            self.new_scores.append(self.scores[idx])\n",
        "            self.new_scores.append(self.scores[idx])\n",
        "            self.new_scores.append( self.scores[idx])\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "#nltk.download()\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "dataset = Dataset(\"/content/drive/My Drive/Colab Notebooks/NLP/data/training_data.csv\")\n",
        "print(dataset.text.shape)\n",
        "dataset.preprocess()\n",
        "dataset.create_vocab()\n",
        "# print(len(dataset.vocab))\n",
        "# print(dataset.vocab[:10])\n",
        "# print(dataset.word_to_id['!'])\n",
        "# dataset.create_chunks()\n",
        "# print(dataset.new_data.shape)\n",
        "# print(dataset.text[0])\n",
        "\"\"\"\n",
        "print(len(dataset.new_data), len(dataset.essay_id))\n",
        "print(\"0: \", dataset.new_data[0, 0])\n",
        "print(\"1: \", dataset.new_data[1, 0])\n",
        "print(\"2: \", dataset.new_data[2, 0])\n",
        "print(dataset.text[0])\n",
        "\"\"\""
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(723,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(len(dataset.new_data), len(dataset.essay_id))\\nprint(\"0: \", dataset.new_data[0, 0])\\nprint(\"1: \", dataset.new_data[1, 0])\\nprint(\"2: \", dataset.new_data[2, 0])\\nprint(dataset.text[0])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k74Xz8gShd0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.text_num()\n",
        "dataset.create_chunks()\n",
        "train_text = dataset.new_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mnweWEuivIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 0\n",
        "for row in train_x:\n",
        "  leng = len(row[0])\n",
        "  if leng > max_len:\n",
        "    max_len = leng"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY4H6rkwlEse",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "604687ba-189c-4685-c179-85c0dfae1cb1"
      },
      "source": [
        "max_len"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "185"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-WromJsl6Ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text = [train_text[i][0] for i in range(len(train_text))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGRgK_Xei78d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padding with space (0)\n",
        "for i in range(len(train_text)):\n",
        "    while len(train_text[i])<=max_len:\n",
        "        train_text[i].append(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbsMUIRtiSwW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d6565470-55dc-434c-f505-727c42f4b02d"
      },
      "source": [
        "train_label = dataset.new_scores\n",
        "len(train_label)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2169"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4maWqmMAdykt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_text, train_label)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0IJEwHVlmFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74cfba06-9824-4b10-8832-92cabb0d3cd5"
      },
      "source": [
        "train_x[0]"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10208,\n",
              " 4047,\n",
              " 6612,\n",
              " 6203,\n",
              " 5094,\n",
              " 2590,\n",
              " 11488,\n",
              " 5094,\n",
              " 43,\n",
              " 4487,\n",
              " 10437,\n",
              " 5410,\n",
              " 1323,\n",
              " 43,\n",
              " 6681,\n",
              " 11142,\n",
              " 6610,\n",
              " 11865,\n",
              " 8531,\n",
              " 9289,\n",
              " 62,\n",
              " 5401,\n",
              " 9304,\n",
              " 10315,\n",
              " 4532,\n",
              " 35,\n",
              " 12104,\n",
              " 2150,\n",
              " 9731,\n",
              " 35,\n",
              " 382,\n",
              " 662,\n",
              " 9757,\n",
              " 43,\n",
              " 11252,\n",
              " 6381,\n",
              " 6217,\n",
              " 43,\n",
              " 12107,\n",
              " 2150,\n",
              " 9731,\n",
              " 35,\n",
              " 2460,\n",
              " 7138,\n",
              " 1371,\n",
              " 43,\n",
              " 6225,\n",
              " 5094,\n",
              " 0,\n",
              " 7324,\n",
              " 2460,\n",
              " 6640,\n",
              " 10519,\n",
              " 6225,\n",
              " 63,\n",
              " 12409,\n",
              " 6037,\n",
              " 8925,\n",
              " 3797,\n",
              " 5969,\n",
              " 5145,\n",
              " 8,\n",
              " 10409,\n",
              " 6225,\n",
              " 3781,\n",
              " 43,\n",
              " 380,\n",
              " 5483,\n",
              " 7089,\n",
              " 12107,\n",
              " 3465,\n",
              " 43,\n",
              " 2145,\n",
              " 6621,\n",
              " 12377,\n",
              " 63,\n",
              " 12127,\n",
              " 8507,\n",
              " 11307,\n",
              " 9478,\n",
              " 6405,\n",
              " 8660,\n",
              " 9237,\n",
              " 9418,\n",
              " 435,\n",
              " 11261,\n",
              " 43,\n",
              " 6217,\n",
              " 2590,\n",
              " 43,\n",
              " 8349,\n",
              " 11038,\n",
              " 6920,\n",
              " 10541,\n",
              " 0,\n",
              " 11675,\n",
              " 43,\n",
              " 6647,\n",
              " 6366,\n",
              " 6811,\n",
              " 4487,\n",
              " 10146,\n",
              " 7233,\n",
              " 4383,\n",
              " 43,\n",
              " 11549,\n",
              " 6121,\n",
              " 4886,\n",
              " 6365,\n",
              " 63,\n",
              " 7857,\n",
              " 12409,\n",
              " 6065,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUcxUrqCjkJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_seq_x = np.array(train_x)\n",
        "valid_seq_x = np.array(valid_x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgD4Srifkjty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_y = np.array(train_y)[:,0]\n",
        "valid_y = np.array(valid_y)[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGkUkhtRf7G7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "dd368844-08e6-4294-9b57-643c613bd982"
      },
      "source": [
        "vocab_len = len(dataset.vocab)\n",
        "vocab_len"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12527"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYzJG9s59CDr",
        "colab_type": "code",
        "outputId": "73f5cf59-7029-4580-d240-da1c85a18574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(train_seq_x.shape, train_y.shape)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1626, 186) (1626,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSqIXoaqmSaz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "065982e2-5fdb-4040-db08-23ca2c5547a4"
      },
      "source": [
        "print(valid_seq_x.shape, valid_y.shape)"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(543, 186) (543,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H1EAlvu9VdO",
        "colab_type": "code",
        "outputId": "36063f6a-6ad7-4e86-cb94-b54cd3c58365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "train_seq_x"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10208,  4047,  6612, ...,     0,     0,     0],\n",
              "       [ 5044,    35,  4116, ...,     0,     0,     0],\n",
              "       [ 6237,    43,  4487, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [10436,    43,  2080, ...,     0,     0,     0],\n",
              "       [ 3581,  4976,  6980, ...,     0,     0,     0],\n",
              "       [   43,  6237,  2010, ...,     0,     0,     0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-bExpfnd-5I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "be26452f-48af-4274-bbb3-7b734a8ae216"
      },
      "source": [
        "train_y = train_y.astype(float)\n",
        "valid_y = valid_y.astype(float)\n",
        "train_y"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4., 4., 4., ..., 4., 5., 2.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gYhZoLZXLos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6ADZ04WWqnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# device = torch.device('cpu')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_seq_x1 = torch.from_numpy(train_seq_x).to(device)\n",
        "train_y1 = torch.from_numpy(train_y).to(device)\n",
        "\n",
        "valid_seq_x1 = torch.from_numpy(valid_seq_x).to(device)\n",
        "valid_y1 = torch.from_numpy(valid_y).to(device)\n",
        "\n",
        "\n",
        "batch_size = 50\n",
        "train_loader = DataLoader(TensorDataset(train_seq_x1, train_y1), batch_size = batch_size, shuffle = True)\n",
        "valid_loader = DataLoader(TensorDataset(valid_seq_x1, valid_y1), batch_size = batch_size, shuffle = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qvmSDaJjxgA",
        "colab_type": "code",
        "outputId": "13b81bb2-c6f0-4cb8-a05d-6a10b7078a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train_seq_x1.shape, train_y1.shape"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1626, 186]), torch.Size([1626]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhZ1IFVnjVBW",
        "colab_type": "code",
        "outputId": "0e5fa344-ded2-4897-e68e-16f2ac67e4b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "device"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FFL2oRnVW2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class LSTM_Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_num, output_num, layer_num):\n",
        "      super().__init__()\n",
        "      self.vocab_size = vocab_size\n",
        "      self.layer_num = layer_num\n",
        "      self.hidden_num = hidden_num\n",
        "\n",
        "      self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "      self.lstm = nn.LSTM(embedding_size, hidden_num, layer_num)\n",
        "      self.fc = nn.Linear(hidden_num, output_num)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "        \n",
        "    def forward(self, word_seq):\n",
        "      word_emb = self.embedding(word_seq)\n",
        "      lstm_out,h = self.lstm(word_emb)\n",
        "      lstm_out = lstm_out.contiguous().view(-1, self.hidden_num)\n",
        "      fc_out = self.fc(lstm_out)\n",
        "      relu_out = self.relu(fc_out)\n",
        "      relu_out = relu_out.view(batch_size, -1) \n",
        "      relu_out = relu_out[:,-1]\n",
        "      return relu_out, h\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZNi35bKWXt7",
        "colab_type": "code",
        "outputId": "701506ab-d8dc-4c9e-f130-fcbbf9c548a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "train_seq_x.shape"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1626, 186)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52hmyt9hVW_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(data_loader, classifier, loss_function, optimizer):\n",
        "    classifier.train()\n",
        "    loss = 0\n",
        "    losses = []\n",
        "    prediction_list = []\n",
        "    accuracy = 0\n",
        "    accuracies = []\n",
        "    for i, (texts, labels) in enumerate(data_loader):\n",
        "      \n",
        "        if(texts.shape[0] != batch_size):\n",
        "            break\n",
        "        labels = labels.float()\n",
        "        texts = texts.cuda()\n",
        "        labels = labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        predictions,h = classifier(texts)\n",
        "        # print(predictions.type(), labels.type())\n",
        "        loss = loss_function(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item()) \n",
        "        prediction_list.append(predictions.cpu().data.numpy().tolist())\n",
        "        # print(losses)       \n",
        "    return prediction_list, sum(losses)/len(losses)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAcYyhCiYgPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_vocab = vocab_len + 1000\n",
        "# n_vocab = len(embedding_matrix)\n",
        "n_embed = 300\n",
        "n_hidden = 512\n",
        "n_output = 1\n",
        "n_layers = 2\n",
        "\n",
        "rnn_model = LSTM_Model(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
        "rnn_model.cuda()\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(rnn_model.parameters(), lr=0.0001, momentum=0.9)\n",
        "epochs = 5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpn2meSknDHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(data_loader, classifier, loss_function, optimizer):\n",
        "    classifier.eval()\n",
        "    loss = 0\n",
        "    losses = []\n",
        "    prediction_list = []\n",
        "    accuracy = 0\n",
        "    accuracies = []\n",
        "    for i, (texts, labels) in enumerate(data_loader):\n",
        "      \n",
        "        if(texts.shape[0] != batch_size):\n",
        "            break\n",
        "        labels = labels.float()\n",
        "        texts = texts.cuda()\n",
        "        labels = labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        predictions,h = classifier(texts)\n",
        "        # print(predictions.type(), labels.type())\n",
        "        loss = loss_function(predictions, labels)\n",
        "        losses.append(loss.item()) \n",
        "        prediction_list.append(predictions.cpu().data.numpy().tolist())\n",
        "        # print(losses)       \n",
        "    return prediction_list, sum(losses)/len(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmt60RhcVW92",
        "colab_type": "code",
        "outputId": "5d29dbf2-e222-4573-9b05-19d2b8936ee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    print(\"epoch:\", epoch + 1)\n",
        "    train_predictions, training_loss = train(train_loader, rnn_model, loss_function, optimizer)\n",
        "    val_predictions, validation_loss = validation(valid_loader, rnn_model, loss_function, optimizer)\n",
        "    print(\"training_loss:\", training_loss)\n",
        "    print(\"validation_loss:\", validation_loss)\n"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1\n",
            "training_loss: 14.613463133573532\n",
            "validation_loss: 12.651982879638672\n",
            "epoch: 2\n",
            "training_loss: 9.47992978990078\n",
            "validation_loss: 4.77466197013855\n",
            "epoch: 3\n",
            "training_loss: 2.4285206254571676\n",
            "validation_loss: 0.8275843918323517\n",
            "epoch: 4\n",
            "training_loss: 0.7857179678976536\n",
            "validation_loss: 0.7273426592350006\n",
            "epoch: 5\n",
            "training_loss: 0.7508827205747366\n",
            "validation_loss: 0.8203333258628845\n",
            "epoch: 6\n",
            "training_loss: 0.7747148126363754\n",
            "validation_loss: 0.7306876540184021\n",
            "epoch: 7\n",
            "training_loss: 0.7616728246212006\n",
            "validation_loss: 0.7330506324768067\n",
            "epoch: 8\n",
            "training_loss: 0.7244056519120932\n",
            "validation_loss: 0.7250236392021179\n",
            "epoch: 9\n",
            "training_loss: 0.747045218013227\n",
            "validation_loss: 0.7508744299411774\n",
            "epoch: 10\n",
            "training_loss: 0.7274942751973867\n",
            "validation_loss: 0.7593670934438705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4EGJwSW6axL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f8ed98e-1c6b-47a5-e70c-b7e6e9c31c6a"
      },
      "source": [
        "val_predictions"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.2361981868743896,\n",
              "  2.0562522411346436,\n",
              "  2.678243398666382,\n",
              "  3.1028528213500977,\n",
              "  3.3802244663238525,\n",
              "  3.5580923557281494,\n",
              "  3.6713685989379883,\n",
              "  3.7434167861938477,\n",
              "  3.789320945739746,\n",
              "  3.818667411804199,\n",
              "  3.837514877319336,\n",
              "  3.8496832847595215,\n",
              "  3.857588052749634,\n",
              "  3.8627562522888184,\n",
              "  3.866159200668335,\n",
              "  3.868417263031006,\n",
              "  3.869926691055298,\n",
              "  3.8709449768066406,\n",
              "  3.8716375827789307,\n",
              "  3.87211275100708,\n",
              "  3.872441053390503,\n",
              "  3.8726706504821777,\n",
              "  3.8728320598602295,\n",
              "  3.8729472160339355,\n",
              "  3.8730297088623047,\n",
              "  3.87308931350708,\n",
              "  3.8731324672698975,\n",
              "  3.873164415359497,\n",
              "  3.8731882572174072,\n",
              "  3.8732059001922607,\n",
              "  3.873218536376953,\n",
              "  3.873228073120117,\n",
              "  3.8732354640960693,\n",
              "  3.8732411861419678,\n",
              "  3.8732454776763916,\n",
              "  3.87324857711792,\n",
              "  3.873250961303711,\n",
              "  3.873253107070923,\n",
              "  3.8732545375823975,\n",
              "  3.873255491256714,\n",
              "  3.8732564449310303,\n",
              "  3.873257637023926,\n",
              "  3.873257875442505,\n",
              "  3.873258113861084,\n",
              "  3.873258352279663,\n",
              "  3.873258590698242,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732590675354004],\n",
              " [1.2361981868743896,\n",
              "  2.0562522411346436,\n",
              "  2.678243398666382,\n",
              "  3.1028528213500977,\n",
              "  3.3802244663238525,\n",
              "  3.5580923557281494,\n",
              "  3.6713685989379883,\n",
              "  3.7434167861938477,\n",
              "  3.789320945739746,\n",
              "  3.818667411804199,\n",
              "  3.837514877319336,\n",
              "  3.8496832847595215,\n",
              "  3.857588052749634,\n",
              "  3.8627562522888184,\n",
              "  3.866159200668335,\n",
              "  3.868417263031006,\n",
              "  3.869926691055298,\n",
              "  3.8709449768066406,\n",
              "  3.8716375827789307,\n",
              "  3.87211275100708,\n",
              "  3.872441053390503,\n",
              "  3.8726706504821777,\n",
              "  3.8728320598602295,\n",
              "  3.8729472160339355,\n",
              "  3.8730297088623047,\n",
              "  3.87308931350708,\n",
              "  3.8731324672698975,\n",
              "  3.873164415359497,\n",
              "  3.8731882572174072,\n",
              "  3.8732059001922607,\n",
              "  3.873218536376953,\n",
              "  3.873228073120117,\n",
              "  3.8732354640960693,\n",
              "  3.8732411861419678,\n",
              "  3.8732454776763916,\n",
              "  3.87324857711792,\n",
              "  3.873250961303711,\n",
              "  3.873253107070923,\n",
              "  3.8732545375823975,\n",
              "  3.873255491256714,\n",
              "  3.8732564449310303,\n",
              "  3.873257637023926,\n",
              "  3.873257875442505,\n",
              "  3.873258113861084,\n",
              "  3.873258352279663,\n",
              "  3.873258590698242,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732590675354004],\n",
              " [1.2361981868743896,\n",
              "  2.0562522411346436,\n",
              "  2.678243398666382,\n",
              "  3.1028528213500977,\n",
              "  3.3802244663238525,\n",
              "  3.5580923557281494,\n",
              "  3.6713685989379883,\n",
              "  3.7434167861938477,\n",
              "  3.789320945739746,\n",
              "  3.818667411804199,\n",
              "  3.837514877319336,\n",
              "  3.8496832847595215,\n",
              "  3.857588052749634,\n",
              "  3.8627562522888184,\n",
              "  3.866159200668335,\n",
              "  3.868417263031006,\n",
              "  3.869926691055298,\n",
              "  3.8709449768066406,\n",
              "  3.8716375827789307,\n",
              "  3.87211275100708,\n",
              "  3.872441053390503,\n",
              "  3.8726706504821777,\n",
              "  3.8728320598602295,\n",
              "  3.8729472160339355,\n",
              "  3.8730297088623047,\n",
              "  3.87308931350708,\n",
              "  3.8731324672698975,\n",
              "  3.873164415359497,\n",
              "  3.8731882572174072,\n",
              "  3.8732059001922607,\n",
              "  3.873218536376953,\n",
              "  3.873228073120117,\n",
              "  3.8732354640960693,\n",
              "  3.8732411861419678,\n",
              "  3.8732454776763916,\n",
              "  3.87324857711792,\n",
              "  3.873250961303711,\n",
              "  3.873253107070923,\n",
              "  3.8732545375823975,\n",
              "  3.873255491256714,\n",
              "  3.8732564449310303,\n",
              "  3.873257637023926,\n",
              "  3.873257875442505,\n",
              "  3.873258113861084,\n",
              "  3.873258352279663,\n",
              "  3.873258590698242,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732590675354004],\n",
              " [1.2361981868743896,\n",
              "  2.0562522411346436,\n",
              "  2.678243398666382,\n",
              "  3.1028528213500977,\n",
              "  3.3802244663238525,\n",
              "  3.5580923557281494,\n",
              "  3.6713685989379883,\n",
              "  3.7434167861938477,\n",
              "  3.789320945739746,\n",
              "  3.818667411804199,\n",
              "  3.837514877319336,\n",
              "  3.8496832847595215,\n",
              "  3.857588052749634,\n",
              "  3.8627562522888184,\n",
              "  3.866159200668335,\n",
              "  3.868417263031006,\n",
              "  3.869926691055298,\n",
              "  3.8709449768066406,\n",
              "  3.8716375827789307,\n",
              "  3.87211275100708,\n",
              "  3.872441053390503,\n",
              "  3.8726706504821777,\n",
              "  3.8728320598602295,\n",
              "  3.8729472160339355,\n",
              "  3.8730297088623047,\n",
              "  3.87308931350708,\n",
              "  3.8731324672698975,\n",
              "  3.873164415359497,\n",
              "  3.8731882572174072,\n",
              "  3.8732059001922607,\n",
              "  3.873218536376953,\n",
              "  3.873228073120117,\n",
              "  3.8732354640960693,\n",
              "  3.8732411861419678,\n",
              "  3.8732454776763916,\n",
              "  3.87324857711792,\n",
              "  3.873250961303711,\n",
              "  3.873253107070923,\n",
              "  3.8732545375823975,\n",
              "  3.873255491256714,\n",
              "  3.8732564449310303,\n",
              "  3.873257637023926,\n",
              "  3.873257875442505,\n",
              "  3.873258113861084,\n",
              "  3.873258352279663,\n",
              "  3.873258590698242,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732590675354004],\n",
              " [1.2361981868743896,\n",
              "  2.0562522411346436,\n",
              "  2.678243398666382,\n",
              "  3.1028528213500977,\n",
              "  3.3802244663238525,\n",
              "  3.5580923557281494,\n",
              "  3.6713685989379883,\n",
              "  3.7434167861938477,\n",
              "  3.789320945739746,\n",
              "  3.818667411804199,\n",
              "  3.837514877319336,\n",
              "  3.8496832847595215,\n",
              "  3.857588052749634,\n",
              "  3.8627562522888184,\n",
              "  3.866159200668335,\n",
              "  3.868417263031006,\n",
              "  3.869926691055298,\n",
              "  3.8709449768066406,\n",
              "  3.8716375827789307,\n",
              "  3.87211275100708,\n",
              "  3.872441053390503,\n",
              "  3.8726706504821777,\n",
              "  3.8728320598602295,\n",
              "  3.8729472160339355,\n",
              "  3.8730297088623047,\n",
              "  3.87308931350708,\n",
              "  3.8731324672698975,\n",
              "  3.873164415359497,\n",
              "  3.8731882572174072,\n",
              "  3.8732059001922607,\n",
              "  3.873218536376953,\n",
              "  3.873228073120117,\n",
              "  3.8732354640960693,\n",
              "  3.8732411861419678,\n",
              "  3.8732454776763916,\n",
              "  3.87324857711792,\n",
              "  3.873250961303711,\n",
              "  3.873253107070923,\n",
              "  3.8732545375823975,\n",
              "  3.873255491256714,\n",
              "  3.8732564449310303,\n",
              "  3.873257637023926,\n",
              "  3.873257875442505,\n",
              "  3.873258113861084,\n",
              "  3.873258352279663,\n",
              "  3.873258590698242,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732590675354004],\n",
              " [1.2361981868743896,\n",
              "  2.0562522411346436,\n",
              "  2.678243398666382,\n",
              "  3.1028528213500977,\n",
              "  3.3802244663238525,\n",
              "  3.5580923557281494,\n",
              "  3.6713685989379883,\n",
              "  3.7434167861938477,\n",
              "  3.789320945739746,\n",
              "  3.818667411804199,\n",
              "  3.837514877319336,\n",
              "  3.8496832847595215,\n",
              "  3.857588052749634,\n",
              "  3.8627562522888184,\n",
              "  3.866159200668335,\n",
              "  3.868417263031006,\n",
              "  3.869926691055298,\n",
              "  3.8709449768066406,\n",
              "  3.8716375827789307,\n",
              "  3.87211275100708,\n",
              "  3.872441053390503,\n",
              "  3.8726706504821777,\n",
              "  3.8728320598602295,\n",
              "  3.8729472160339355,\n",
              "  3.8730297088623047,\n",
              "  3.87308931350708,\n",
              "  3.8731324672698975,\n",
              "  3.873164415359497,\n",
              "  3.8731882572174072,\n",
              "  3.8732059001922607,\n",
              "  3.873218536376953,\n",
              "  3.873228073120117,\n",
              "  3.8732354640960693,\n",
              "  3.8732411861419678,\n",
              "  3.8732454776763916,\n",
              "  3.87324857711792,\n",
              "  3.873250961303711,\n",
              "  3.873253107070923,\n",
              "  3.8732545375823975,\n",
              "  3.873255491256714,\n",
              "  3.8732564449310303,\n",
              "  3.873257637023926,\n",
              "  3.873257875442505,\n",
              "  3.873258113861084,\n",
              "  3.873258352279663,\n",
              "  3.873258590698242,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732590675354004],\n",
              " [1.2361981868743896,\n",
              "  2.0562522411346436,\n",
              "  2.678243398666382,\n",
              "  3.1028528213500977,\n",
              "  3.3802244663238525,\n",
              "  3.5580923557281494,\n",
              "  3.6713685989379883,\n",
              "  3.7434167861938477,\n",
              "  3.789320945739746,\n",
              "  3.818667411804199,\n",
              "  3.837514877319336,\n",
              "  3.8496832847595215,\n",
              "  3.857588052749634,\n",
              "  3.8627562522888184,\n",
              "  3.866159200668335,\n",
              "  3.868417263031006,\n",
              "  3.869926691055298,\n",
              "  3.8709449768066406,\n",
              "  3.8716375827789307,\n",
              "  3.87211275100708,\n",
              "  3.872441053390503,\n",
              "  3.8726706504821777,\n",
              "  3.8728320598602295,\n",
              "  3.8729472160339355,\n",
              "  3.8730297088623047,\n",
              "  3.87308931350708,\n",
              "  3.8731324672698975,\n",
              "  3.873164415359497,\n",
              "  3.8731882572174072,\n",
              "  3.8732059001922607,\n",
              "  3.873218536376953,\n",
              "  3.873228073120117,\n",
              "  3.8732354640960693,\n",
              "  3.8732411861419678,\n",
              "  3.8732454776763916,\n",
              "  3.87324857711792,\n",
              "  3.873250961303711,\n",
              "  3.873253107070923,\n",
              "  3.8732545375823975,\n",
              "  3.873255491256714,\n",
              "  3.8732564449310303,\n",
              "  3.873257637023926,\n",
              "  3.873257875442505,\n",
              "  3.873258113861084,\n",
              "  3.873258352279663,\n",
              "  3.873258590698242,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732590675354004],\n",
              " [1.2361981868743896,\n",
              "  2.0562522411346436,\n",
              "  2.678243398666382,\n",
              "  3.1028528213500977,\n",
              "  3.3802244663238525,\n",
              "  3.5580923557281494,\n",
              "  3.6713685989379883,\n",
              "  3.7434167861938477,\n",
              "  3.789320945739746,\n",
              "  3.818667411804199,\n",
              "  3.837514877319336,\n",
              "  3.8496832847595215,\n",
              "  3.857588052749634,\n",
              "  3.8627562522888184,\n",
              "  3.866159200668335,\n",
              "  3.868417263031006,\n",
              "  3.869926691055298,\n",
              "  3.8709449768066406,\n",
              "  3.8716375827789307,\n",
              "  3.87211275100708,\n",
              "  3.872441053390503,\n",
              "  3.8726706504821777,\n",
              "  3.8728320598602295,\n",
              "  3.8729472160339355,\n",
              "  3.8730297088623047,\n",
              "  3.87308931350708,\n",
              "  3.8731324672698975,\n",
              "  3.873164415359497,\n",
              "  3.8731882572174072,\n",
              "  3.8732059001922607,\n",
              "  3.873218536376953,\n",
              "  3.873228073120117,\n",
              "  3.8732354640960693,\n",
              "  3.8732411861419678,\n",
              "  3.8732454776763916,\n",
              "  3.87324857711792,\n",
              "  3.873250961303711,\n",
              "  3.873253107070923,\n",
              "  3.8732545375823975,\n",
              "  3.873255491256714,\n",
              "  3.8732564449310303,\n",
              "  3.873257637023926,\n",
              "  3.873257875442505,\n",
              "  3.873258113861084,\n",
              "  3.873258352279663,\n",
              "  3.873258590698242,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732590675354004],\n",
              " [1.2361981868743896,\n",
              "  2.0562522411346436,\n",
              "  2.678243398666382,\n",
              "  3.1028528213500977,\n",
              "  3.3802244663238525,\n",
              "  3.5580923557281494,\n",
              "  3.6713685989379883,\n",
              "  3.7434167861938477,\n",
              "  3.789320945739746,\n",
              "  3.818667411804199,\n",
              "  3.837514877319336,\n",
              "  3.8496832847595215,\n",
              "  3.857588052749634,\n",
              "  3.8627562522888184,\n",
              "  3.866159200668335,\n",
              "  3.868417263031006,\n",
              "  3.869926691055298,\n",
              "  3.8709449768066406,\n",
              "  3.8716375827789307,\n",
              "  3.87211275100708,\n",
              "  3.872441053390503,\n",
              "  3.8726706504821777,\n",
              "  3.8728320598602295,\n",
              "  3.8729472160339355,\n",
              "  3.8730297088623047,\n",
              "  3.87308931350708,\n",
              "  3.8731324672698975,\n",
              "  3.873164415359497,\n",
              "  3.8731882572174072,\n",
              "  3.8732059001922607,\n",
              "  3.873218536376953,\n",
              "  3.873228073120117,\n",
              "  3.8732354640960693,\n",
              "  3.8732411861419678,\n",
              "  3.8732454776763916,\n",
              "  3.87324857711792,\n",
              "  3.873250961303711,\n",
              "  3.873253107070923,\n",
              "  3.8732545375823975,\n",
              "  3.873255491256714,\n",
              "  3.8732564449310303,\n",
              "  3.873257637023926,\n",
              "  3.873257875442505,\n",
              "  3.873258113861084,\n",
              "  3.873258352279663,\n",
              "  3.873258590698242,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732590675354004],\n",
              " [1.2361981868743896,\n",
              "  2.0562522411346436,\n",
              "  2.678243398666382,\n",
              "  3.1028528213500977,\n",
              "  3.3802244663238525,\n",
              "  3.5580923557281494,\n",
              "  3.6713685989379883,\n",
              "  3.7434167861938477,\n",
              "  3.789320945739746,\n",
              "  3.818667411804199,\n",
              "  3.837514877319336,\n",
              "  3.8496832847595215,\n",
              "  3.857588052749634,\n",
              "  3.8627562522888184,\n",
              "  3.866159200668335,\n",
              "  3.868417263031006,\n",
              "  3.869926691055298,\n",
              "  3.8709449768066406,\n",
              "  3.8716375827789307,\n",
              "  3.87211275100708,\n",
              "  3.872441053390503,\n",
              "  3.8726706504821777,\n",
              "  3.8728320598602295,\n",
              "  3.8729472160339355,\n",
              "  3.8730297088623047,\n",
              "  3.87308931350708,\n",
              "  3.8731324672698975,\n",
              "  3.873164415359497,\n",
              "  3.8731882572174072,\n",
              "  3.8732059001922607,\n",
              "  3.873218536376953,\n",
              "  3.873228073120117,\n",
              "  3.8732354640960693,\n",
              "  3.8732411861419678,\n",
              "  3.8732454776763916,\n",
              "  3.87324857711792,\n",
              "  3.873250961303711,\n",
              "  3.873253107070923,\n",
              "  3.8732545375823975,\n",
              "  3.873255491256714,\n",
              "  3.8732564449310303,\n",
              "  3.873257637023926,\n",
              "  3.873257875442505,\n",
              "  3.873258113861084,\n",
              "  3.873258352279663,\n",
              "  3.873258590698242,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732588291168213,\n",
              "  3.8732590675354004]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9MPQlXH8nVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKAndDymnJrS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}